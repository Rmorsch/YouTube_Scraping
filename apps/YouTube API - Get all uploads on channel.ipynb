{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "35a80a92d92fe2a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.106740Z",
     "start_time": "2025-07-05T00:16:40.096760Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, time, math, itertools, json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from tqdm import tqdm   # progress bar\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote_plus  # safely URL-encode the driver name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4363f0c04d5152d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.146351Z",
     "start_time": "2025-07-05T00:16:40.139770Z"
    }
   },
   "outputs": [],
   "source": [
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")          # export beforehand or load via dotenv\n",
    "if not API_KEY:\n",
    "    raise SystemExit(\"Set YOUTUBE_API_KEY environment variable first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "65886f974c2a75d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.186965Z",
     "start_time": "2025-07-05T00:16:40.179662Z"
    }
   },
   "outputs": [],
   "source": [
    "def chunks(seq, n):\n",
    "    \"\"\"Yield successive n-sized chunks from seq (used for video-id batching).\"\"\"\n",
    "    for i in range(0, len(seq), n):\n",
    "        yield seq[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1ce3fe8b88b05971",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.240790Z",
     "start_time": "2025-07-05T00:16:40.233267Z"
    }
   },
   "outputs": [],
   "source": [
    "def safe_get(item, path, default=None):\n",
    "    \"\"\"Safely drill into nested dicts.\"\"\"\n",
    "    for key in path:\n",
    "        item = item.get(key, {})\n",
    "    return item or default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "158fa7d97464c615",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.282885Z",
     "start_time": "2025-07-05T00:16:40.275439Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_service():\n",
    "    # https://googleapis.github.io/google-api-python-client/docs/epy/googleapiclient.discovery-module.html#build\n",
    "    # build(serviceName, version, developerKey=None, cache_discovery=True)\n",
    "    #\n",
    "    # serviceName: string, name of the service.\n",
    "    # The serviceName and version are the names from the Discovery service.\n",
    "    #\n",
    "    # cache_discovery: Boolean, whether or not to cache the discovery doc.\n",
    "    #\n",
    "\n",
    "    return build(\"youtube\", \"v3\", developerKey=API_KEY, cache_discovery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c6a57ccd2a998663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.335936Z",
     "start_time": "2025-07-05T00:16:40.324798Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_uploads_playlist_id(youtube, channel_id):\n",
    "    \"\"\"Step 1: one cheap call â†’ uploads playlistId.\"\"\"\n",
    "    resp = youtube.channels().list(\n",
    "        part=\"contentDetails\",\n",
    "        id=channel_id,\n",
    "        maxResults=1\n",
    "    ).execute()\n",
    "    try:\n",
    "        return resp[\"items\"][0][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "    except (KeyError, IndexError):\n",
    "        raise ValueError(\"Channel ID not found or no public uploads.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7a0ab04d1a496882",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.379722Z",
     "start_time": "2025-07-05T00:16:40.369404Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_video_ids(youtube, uploads_playlist_id):\n",
    "    \"\"\"Step 2: page through playlistItems; collect videoIds.\"\"\"\n",
    "    video_ids = []\n",
    "    next_page = None\n",
    "    pbar = tqdm(desc=\"Fetching playlist pages\", unit=\"page\")\n",
    "    while True:\n",
    "        resp = youtube.playlistItems().list(\n",
    "            part=\"contentDetails\",\n",
    "            playlistId=uploads_playlist_id,\n",
    "            maxResults=50,      # API max\n",
    "            pageToken=next_page\n",
    "        ).execute()\n",
    "        ids = [item[\"contentDetails\"][\"videoId\"] for item in resp[\"items\"]]\n",
    "        video_ids.extend(ids)\n",
    "        pbar.update(1)\n",
    "        next_page = resp.get(\"nextPageToken\")\n",
    "        if not next_page:\n",
    "            break\n",
    "    pbar.close()\n",
    "    return video_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3e22b81f02c89c64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.407967Z",
     "start_time": "2025-07-05T00:16:40.400026Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_video_metadata(youtube, video_ids):\n",
    "    \"\"\"Step 3: batch-fetch videos.list in groups of â‰¤50 ids.\"\"\"\n",
    "    rows = []\n",
    "    for batch in tqdm(list(chunks(video_ids, 50)), desc=\"Downloading metadata\", unit=\"batch\"):\n",
    "        resp = youtube.videos().list(\n",
    "            part=\"snippet,statistics,contentDetails\",\n",
    "            id=\",\".join(batch),\n",
    "            maxResults=50\n",
    "        ).execute()\n",
    "        for v in resp[\"items\"]:\n",
    "            sni, stats, cd = v[\"snippet\"], v[\"statistics\"], v[\"contentDetails\"]\n",
    "            rows.append({\n",
    "                \"video_id\"      : v[\"id\"],\n",
    "                \"title\"         : sni.get(\"title\"),\n",
    "                \"published_at\"  : sni.get(\"publishedAt\"),\n",
    "                \"description\"   : sni.get(\"description\"),\n",
    "                \"duration_ISO\"  : cd.get(\"duration\"),       # e.g. PT13M20S\n",
    "                \"tags\"          : \"|\".join(sni.get(\"tags\", [])),\n",
    "                \"view_count\"    : int(stats.get(\"viewCount\", 0)),\n",
    "                \"like_count\"    : int(stats.get(\"likeCount\", 0)),\n",
    "                \"comment_count\" : int(stats.get(\"commentCount\", 0)),\n",
    "                \"favorite_count\": int(stats.get(\"favoriteCount\", 0)),\n",
    "                \"channel_title\" : sni.get(\"channelTitle\")\n",
    "            })\n",
    "        # polite pause â€“ keeps you well below quota & QPS limits\n",
    "        time.sleep(0.1)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8f701925e7ec31e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.440671Z",
     "start_time": "2025-07-05T00:16:40.432569Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_dataframe(df):\n",
    "    df[\"published_at\"] = (\n",
    "        pd.to_datetime(df[\"published_at\"], utc=True)  # parse ISO-8601\n",
    "          .dt.tz_convert(None)                       # drop the UTC tz-info\n",
    "    )\n",
    "\n",
    "    # --- make sure numeric cols are true ints, not NaN/float strings ----------\n",
    "    num_cols = [\"view_count\", \"like_count\", \"comment_count\", \"favorite_count\"]\n",
    "    df[num_cols] = (\n",
    "        df[num_cols]\n",
    "          .fillna(0)            # YouTube may omit like_count, etc. -> NaN\n",
    "          .astype(\"Int64\")      # pandas nullable int â†’ SQL BIGINT/INT fine\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "950754d9f4f70aad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.467131Z",
     "start_time": "2025-07-05T00:16:40.459036Z"
    }
   },
   "outputs": [],
   "source": [
    "def df_to_azure_sql(df):\n",
    "    \"\"\"\n",
    "    Write/append the dataframe into dbo.youtube_videos (Azure SQL DB)\n",
    "    \"\"\"\n",
    "\n",
    "    drv = \"ODBC Driver 18 for SQL Server\"            # keep spaces!\n",
    "    odbc_str = (\n",
    "        f\"Driver={drv};Server=tcp:{os.getenv('AZSQL_SERVER')},1433;\"\n",
    "        f\"Database={os.getenv('AZSQL_DATABASE')};\"\n",
    "        f\"Uid={os.getenv('AZSQL_USERNAME')};\"\n",
    "        f\"Pwd={os.getenv('AZSQL_PASSWORD')};\"\n",
    "        \"Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\"\n",
    "    )\n",
    "\n",
    "    # SQLAlchemy-style URL.  Space â†’ + ;  parentheses â†’ %28 %29, etc.\n",
    "    params = quote_plus(odbc_str)\n",
    "    engine = create_engine(\n",
    "        f\"mssql+pyodbc:///?odbc_connect={params}\",\n",
    "        fast_executemany=True        # batches rows under the hood\n",
    "    )\n",
    "\n",
    "    # â€”â€” upsert strategy: try append-only, let PK skip duplicates\n",
    "    with engine.begin() as cn:\n",
    "        df.to_sql(\n",
    "            name=\"Vaush_VIDEOS\",\n",
    "            con=cn,\n",
    "            schema=\"YOUTUBE_API\",\n",
    "            if_exists=\"append\",       # create once, then append\n",
    "            index=False,\n",
    "            chunksize=1000,           # good balance of  network / TX\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c2796289b45b0900",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.493860Z",
     "start_time": "2025-07-05T00:16:40.489162Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(channel_id):\n",
    "    #\n",
    "    youtube = build_service()\n",
    "    uploads_id = get_uploads_playlist_id(youtube, channel_id)\n",
    "    print(f\"Uploads playlist ID: {uploads_id}\")\n",
    "    ids = get_all_video_ids(youtube, uploads_id)\n",
    "    print(f\"Total videos: {len(ids):,}\")\n",
    "    df = fetch_video_metadata(youtube, ids)\n",
    "    df = normalize_dataframe(df)\n",
    "    df_to_azure_sql(df)\n",
    "    print(\"Data pushed to Azure SQL ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "340aa9c35fd15f28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.517535Z",
     "start_time": "2025-07-05T00:16:40.514080Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\" and 1==0:\n",
    "    if len(sys.argv) != 2:\n",
    "        raise SystemExit(f\"Usage: python fetch_channel_videos.py <CHANNEL_ID>\")\n",
    "    main(sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "54e4d1ff59e794bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.549524Z",
     "start_time": "2025-07-05T00:16:40.542715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UC1E-JS8L0j1Ei70D9VEFrPQ\n"
     ]
    }
   ],
   "source": [
    "CHANNEL_ID=os.getenv(\"VAUSH_CHANNEL_ID\")\n",
    "print(CHANNEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dfedf986d9aeb12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:17:04.793044Z",
     "start_time": "2025-07-05T00:16:40.614626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploads playlist ID: UU1E-JS8L0j1Ei70D9VEFrPQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching playlist pages: 62page [00:04, 13.94page/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos: 3,091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading metadata: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:13<00:00,  4.48batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pushed to Azure SQL ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "main(CHANNEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691043ac1b0b45a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:17:04.849010Z",
     "start_time": "2025-07-05T00:17:04.842851Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
