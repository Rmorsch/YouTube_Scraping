{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35a80a92d92fe2a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.106740Z",
     "start_time": "2025-07-05T00:16:40.096760Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, time, math, itertools, json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from tqdm import tqdm   # progress bar\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote_plus  # safely URL-encode the driver name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4363f0c04d5152d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.146351Z",
     "start_time": "2025-07-05T00:16:40.139770Z"
    }
   },
   "outputs": [],
   "source": [
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")          # export beforehand or load via dotenv\n",
    "if not API_KEY:\n",
    "    raise SystemExit(\"Set YOUTUBE_API_KEY environment variable first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65886f974c2a75d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.186965Z",
     "start_time": "2025-07-05T00:16:40.179662Z"
    }
   },
   "outputs": [],
   "source": [
    "def chunks(seq, n):\n",
    "    \"\"\"Yield successive n-sized chunks from seq (used for video-id batching).\"\"\"\n",
    "    for i in range(0, len(seq), n):\n",
    "        yield seq[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ce3fe8b88b05971",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.240790Z",
     "start_time": "2025-07-05T00:16:40.233267Z"
    }
   },
   "outputs": [],
   "source": [
    "def safe_get(item, path, default=None):\n",
    "    \"\"\"Safely drill into nested dicts.\"\"\"\n",
    "    for key in path:\n",
    "        item = item.get(key, {})\n",
    "    return item or default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "158fa7d97464c615",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.282885Z",
     "start_time": "2025-07-05T00:16:40.275439Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_service():\n",
    "    # https://googleapis.github.io/google-api-python-client/docs/epy/googleapiclient.discovery-module.html#build\n",
    "    # build(serviceName, version, developerKey=None, cache_discovery=True)\n",
    "    #\n",
    "    # serviceName: string, name of the service.\n",
    "    # The serviceName and version are the names from the Discovery service.\n",
    "    #\n",
    "    # cache_discovery: Boolean, whether or not to cache the discovery doc.\n",
    "    #\n",
    "\n",
    "    return build(\"youtube\", \"v3\", developerKey=API_KEY, cache_discovery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6a57ccd2a998663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.335936Z",
     "start_time": "2025-07-05T00:16:40.324798Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_uploads_playlist_id(youtube, channel_id):\n",
    "    \"\"\"Step 1: one cheap call â†’ uploads playlistId.\"\"\"\n",
    "    resp = youtube.channels().list(\n",
    "        part=\"contentDetails\",\n",
    "        id=channel_id,\n",
    "        maxResults=1\n",
    "    ).execute()\n",
    "    try:\n",
    "        return resp[\"items\"][0][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "    except (KeyError, IndexError):\n",
    "        raise ValueError(\"Channel ID not found or no public uploads.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a0ab04d1a496882",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.379722Z",
     "start_time": "2025-07-05T00:16:40.369404Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_video_ids(youtube, uploads_playlist_id):\n",
    "    \"\"\"Step 2: page through playlistItems; collect videoIds.\"\"\"\n",
    "    video_ids = []\n",
    "    next_page = None\n",
    "    pbar = tqdm(desc=\"Fetching playlist pages\", unit=\"page\")\n",
    "    while True:\n",
    "        resp = youtube.playlistItems().list(\n",
    "            part=\"contentDetails\",\n",
    "            playlistId=uploads_playlist_id,\n",
    "            maxResults=50,      # API max\n",
    "            pageToken=next_page\n",
    "        ).execute()\n",
    "        ids = [item[\"contentDetails\"][\"videoId\"] for item in resp[\"items\"]]\n",
    "        video_ids.extend(ids)\n",
    "        pbar.update(1)\n",
    "        next_page = resp.get(\"nextPageToken\")\n",
    "        if not next_page:\n",
    "            break\n",
    "    pbar.close()\n",
    "    return video_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e22b81f02c89c64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.407967Z",
     "start_time": "2025-07-05T00:16:40.400026Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_video_metadata(youtube, video_ids):\n",
    "    \"\"\"Step 3: batch-fetch videos.list in groups of â‰¤50 ids.\"\"\"\n",
    "    rows = []\n",
    "    for batch in tqdm(list(chunks(video_ids, 50)), desc=\"Downloading metadata\", unit=\"batch\"):\n",
    "        resp = youtube.videos().list(\n",
    "            part=\"snippet,statistics,contentDetails\",\n",
    "            id=\",\".join(batch),\n",
    "            maxResults=50\n",
    "        ).execute()\n",
    "        for v in resp[\"items\"]:\n",
    "            sni, stats, cd = v[\"snippet\"], v[\"statistics\"], v[\"contentDetails\"]\n",
    "            rows.append({\n",
    "                \"video_id\"      : v[\"id\"],\n",
    "                \"title\"         : sni.get(\"title\"),\n",
    "                \"published_at\"  : sni.get(\"publishedAt\"),\n",
    "                \"description\"   : sni.get(\"description\"),\n",
    "                \"duration_ISO\"  : cd.get(\"duration\"),       # e.g. PT13M20S\n",
    "                \"tags\"          : \"|\".join(sni.get(\"tags\", [])),\n",
    "                \"view_count\"    : int(stats.get(\"viewCount\", 0)),\n",
    "                \"like_count\"    : int(stats.get(\"likeCount\", 0)),\n",
    "                \"comment_count\" : int(stats.get(\"commentCount\", 0)),\n",
    "                \"favorite_count\": int(stats.get(\"favoriteCount\", 0)),\n",
    "                \"channel_title\" : sni.get(\"channelTitle\")\n",
    "            })\n",
    "        # polite pause â€“ keeps you well below quota & QPS limits\n",
    "        time.sleep(0.1)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f701925e7ec31e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.440671Z",
     "start_time": "2025-07-05T00:16:40.432569Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_dataframe(df):\n",
    "    df[\"published_at\"] = (\n",
    "        pd.to_datetime(df[\"published_at\"], utc=True)  # parse ISO-8601\n",
    "          .dt.tz_convert(None)                       # drop the UTC tz-info\n",
    "    )\n",
    "\n",
    "    # --- make sure numeric cols are true ints, not NaN/float strings ----------\n",
    "    num_cols = [\"view_count\", \"like_count\", \"comment_count\", \"favorite_count\"]\n",
    "    df[num_cols] = (\n",
    "        df[num_cols]\n",
    "          .fillna(0)            # YouTube may omit like_count, etc. -> NaN\n",
    "          .astype(\"Int64\")      # pandas nullable int â†’ SQL BIGINT/INT fine\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71b78371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build engine (same style you've been using)\n",
    "def select_all_azure_sql():\n",
    "    drv = \"ODBC Driver 18 for SQL Server\"\n",
    "    odbc_str = (\n",
    "        f\"DRIVER={{{drv}}};\"\n",
    "        f\"SERVER=tcp:{os.getenv('AZSQL_SERVER')},1433;\"\n",
    "        f\"DATABASE={os.getenv('AZSQL_DATABASE')};\"\n",
    "        f\"UID={os.getenv('AZSQL_USERNAME')};\"\n",
    "        f\"PWD={os.getenv('AZSQL_PASSWORD')};\"\n",
    "        \"Encrypt=yes;\"\n",
    "        \"TrustServerCertificate=no;\"\n",
    "        \"Connection Timeout=30;\"\n",
    "    )\n",
    "\n",
    "    params = quote_plus(odbc_str)\n",
    "    engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "    # read table into pandas\n",
    "    df = pd.read_sql(\"SELECT * FROM YOUTUBE_API.Vaush_VIDEOS\", engine)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e9aaa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_new_videos(df_in_database, df_from_api):\n",
    "    keys = set(df_in_database[\"video_id\"])\n",
    "    df_filtered = df_from_api[~df_from_api[\"video_id\"].isin(keys)]\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "950754d9f4f70aad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.467131Z",
     "start_time": "2025-07-05T00:16:40.459036Z"
    }
   },
   "outputs": [],
   "source": [
    "def df_to_azure_sql(df):\n",
    "    \"\"\"\n",
    "    Write/append the dataframe into dbo.youtube_videos (Azure SQL DB)\n",
    "    \"\"\"\n",
    "\n",
    "    drv = \"ODBC Driver 18 for SQL Server\"            # keep spaces!\n",
    "    odbc_str = (\n",
    "        f\"Driver={drv};Server=tcp:{os.getenv('AZSQL_SERVER')},1433;\"\n",
    "        f\"Database={os.getenv('AZSQL_DATABASE')};\"\n",
    "        f\"Uid={os.getenv('AZSQL_USERNAME')};\"\n",
    "        f\"Pwd={os.getenv('AZSQL_PASSWORD')};\"\n",
    "        \"Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\"\n",
    "    )\n",
    "\n",
    "    # SQLAlchemy-style URL.  Space â†’ + ;  parentheses â†’ %28 %29, etc.\n",
    "    params = quote_plus(odbc_str)\n",
    "    engine = create_engine(\n",
    "        f\"mssql+pyodbc:///?odbc_connect={params}\",\n",
    "        fast_executemany=True        # batches rows under the hood\n",
    "    )\n",
    "\n",
    "    # â€”â€” upsert strategy: try append-only, let PK skip duplicates\n",
    "    with engine.begin() as cn:\n",
    "        df.to_sql(\n",
    "            name=\"Vaush_VIDEOS\",\n",
    "            con=cn,\n",
    "            schema=\"YOUTUBE_API\",\n",
    "            if_exists=\"append\",       # create once, then append\n",
    "            index=False,\n",
    "            chunksize=1000,           # good balance of  network / TX\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c2796289b45b0900",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T00:16:40.493860Z",
     "start_time": "2025-07-05T00:16:40.489162Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(channel_id):\n",
    "    youtube = build_service()\n",
    "    uploads_id = get_uploads_playlist_id(youtube, channel_id)\n",
    "    print(f\"Uploads playlist ID: {uploads_id}\")\n",
    "    ids = get_all_video_ids(youtube, uploads_id)\n",
    "    print(f\"Total videos: {len(ids):,}\")\n",
    "    df = fetch_video_metadata(youtube, ids)\n",
    "    df = normalize_dataframe(df)\n",
    "    df_in_db = select_all_azure_sql()\n",
    "    df_new_rows = filter_new_videos(df_in_db, df)\n",
    "    print(f\"New rows: {df_new_rows.shape[0]}\")\n",
    "    df_to_azure_sql(df_new_rows)\n",
    "    print(\"Data pushed to Azure SQL ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b4cd3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploads playlist ID: UU1E-JS8L0j1Ei70D9VEFrPQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Fetching playlist pages: 69page [00:08,  7.97page/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos: 3,403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading metadata: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69/69 [00:19<00:00,  3.47batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New rows: 6\n",
      "Data pushed to Azure SQL ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "main(os.getenv('VAUSH_CHANNEL_ID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35846aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7f613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a00bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
